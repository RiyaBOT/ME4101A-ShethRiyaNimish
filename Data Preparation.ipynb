{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unnecessary-pavilion",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "### Sheth Riya Nimish\n",
    "### A0176880R\n",
    "e0235287@u.nus.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-deployment",
   "metadata": {},
   "source": [
    "#### This is the code that is used for data featurizing and data preprocessing. The code has been written entirely from scratch by me with the use of existing python libraries like matminer and pymatgen.\n",
    "\n",
    "This code requires user input such as the name of the datafile, whether certain operations should or should not be performed. After the completion of the code a document is saved in the same folder as this code, it stores the summary of everything that took place in data featurizing and data preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-ottawa",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "\n",
    "* [The Workflow Followed in this Code](#wf)\n",
    "* [Importing the Required Libraries](#import)\n",
    "* [Creating a Document to Store the Results](#create)\n",
    "* [Retrieval of Data](#ret)\n",
    "   * [Reading the Data from the Data File](#read)\n",
    "   * [Retrieval of Data from Databases](#databases)\n",
    "* [Quality of Data](#qod)\n",
    "* [Featurizing of the Data](#fet)\n",
    "   * [Featurizing using Matminer](#featurize)\n",
    "   * [Manual Featurizing of the Data](#amf)\n",
    "* [Data Preprocessing](#prep)\n",
    "   * [Data Cleaning](#cleaning)\n",
    "   * [Data Transformation](#transformation)\n",
    "   * [Data Reduction](#reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-conviction",
   "metadata": {},
   "source": [
    "### The Workflow Followed in this Code<a class=\"anchor\" id=\"wf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-discovery",
   "metadata": {},
   "source": [
    "In order to prepare the raw chemical data for machine learning algorithms, the process of data featurizing and data preprocessing is pivotal. Data featurizing is an important step as it helps transform and featurize complicated material attributes into numerical descriptors for data mining. Raw data is also inconsistent, noisy and incomplete. Hence, data preprocessing generates quality data.\n",
    "\n",
    "![title](Pictures/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-philadelphia",
   "metadata": {},
   "source": [
    "### Importing the Required Libraries<a class=\"anchor\" id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import argparse\n",
    "import docx\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-minutes",
   "metadata": {},
   "source": [
    "### Creating a Document to Store the Results of Data Featurizing and Data Preprocessing <a class=\"anchor\" id=\"create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc= docx.Document()\n",
    "mydoc.add_heading('Data Featurizing and Data Preprocessing', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-passing",
   "metadata": {},
   "source": [
    "Supplementary Notes: Ensure there is a column called formula_pretty which contains the formula(i.e the material in the format IrF2 or Fe2O3) Otherwise a little modification of the code is required (each instance of formula_pretty should be replaced with the name of the formula column in the recieved data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-quick",
   "metadata": {},
   "source": [
    "### Retrieval of Data <a class=\"anchor\" id=\"ret\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-candidate",
   "metadata": {},
   "source": [
    "#### Reading the Data from the Data File<a class=\"anchor\" id=\"read\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-period",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename=input(\"Filename:\") #make sure the data file is in the same folder as this code is in\n",
    "fileformat= input(\"Fileformat:\") #supported formats are csv and json\n",
    "\n",
    "materials=[]\n",
    "if fileformat == \"csv\":\n",
    "    df= pd.read_csv(filename)\n",
    "else:\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            materials.append(json.loads(line))\n",
    "    df= pd.DataFrame.from_dict(materials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-moldova",
   "metadata": {},
   "source": [
    "#### Retrieval of the Data from Databases <a class=\"anchor\" id=\"databases\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this section for data retrieval of a database and specifically computational 2d materials database\n",
    "\n",
    "\"\"\"\n",
    "C2DB\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from ase.db import connect\n",
    "count=0\n",
    "db = connect('c2db.db')\n",
    "formulae=[]\n",
    "centerofmass=[] #array but this array has same number of elements\n",
    "kineticenergy=[] #its all 0 for some reason\n",
    "potentialenergy=[]\n",
    "totalenergy=[]\n",
    "#stress=[] #could not be found\n",
    "volume=[]\n",
    "pbc=[]#its an array\n",
    "magmom=[] #gives the local magnetic moments\n",
    "cell=[] #its an array\n",
    "angularmomentum=[]\n",
    "dipolemoment=[]\n",
    "numberofatoms=[]\n",
    "totalmagneticmoment=[]\n",
    "magnetic=[]\n",
    "\n",
    "allatoms=[]\n",
    "\n",
    "for row in db.select():\n",
    "    atoms = row.toatoms()\n",
    "    allatoms.append(atoms)\n",
    "    formulae.append(atoms.get_chemical_formula())\n",
    "    centerofmass.append(atoms.get_center_of_mass())\n",
    "    kineticenergy.append(atoms.get_kinetic_energy())\n",
    "    potentialenergy.append(atoms.get_potential_energy())\n",
    "    totalenergy.append(atoms.get_total_energy())\n",
    "    #totalmagneticmoment.append(atoms.get_magnetic_moment())\n",
    "    #stress.append(atoms.get_stress())\n",
    "    volume.append(atoms.get_volume())\n",
    "    pbc.append(atoms.get_pbc())\n",
    "    #getting all the possible attributes stored in atoms\n",
    "    magmom.append(atoms.get_initial_magnetic_moments())\n",
    "    cell.append(atoms.get_cell())\n",
    "    angularmomentum.append(atoms.get_angular_momentum())\n",
    "    dipolemoment.append(atoms.get_dipole_moment())\n",
    "    numberofatoms.append(len(atoms))\n",
    "    calc= atoms.calc\n",
    "    try:\n",
    "        magnetic.append(calc.get_magnetic_moment())\n",
    "    except:\n",
    "        magnetic.append(0)\n",
    "#print(len(magnetic))\n",
    "c2db_df= pd.DataFrame({'formula': formulae, 'potentialenergy': potentialenergy, 'totalenergy': totalenergy, 'volume': volume, 'numberofatoms': numberofatoms, 'magnetic': magnetic})\n",
    "c2db_df.shape\n",
    "df= c2db_df\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-asthma",
   "metadata": {},
   "source": [
    "#### Quality of Data<a class=\"anchor\" id=\"qod\"></a>\n",
    "This function gives a brief description of the quality of data at hand. The description includes number of rows and columns in the data, type of data stored in the columns, how dependent are the data columns or features on each other, how complete and unique the datasets are and finally the ratio of data to errors. This assists the user to assess the data and detect the irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-deputy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def qualityofdata(df):\n",
    "    \n",
    "    #an alternate way to check for duplicates\n",
    "    \n",
    "    \"\"\"\n",
    "    dups= df.iloc[:, 2].duplicated()\n",
    "    countduplicated=0\n",
    "    for i in range (0, len(dups)):\n",
    "        if dups[i]== True:\n",
    "            countduplicated= countduplicated+1\n",
    "    \"\"\"\n",
    "    countduplicated=0\n",
    "    mydoc.add_heading(\"Quality of Data\", 1)\n",
    "    mydoc.add_heading(\"Size of the Data\", 6)\n",
    "    mydoc.add_heading(\"Number of columns\", 6)\n",
    "    mydoc.add_paragraph(str(df.shape[1]))\n",
    "    mydoc.add_heading(\"Number of rows\", 6)\n",
    "    mydoc.add_paragraph(str(df.shape[0]))\n",
    "    mydoc.add_heading(\"Type of data in each of the columns\", 6)\n",
    "    mydoc.add_paragraph(str(df.dtypes))\n",
    "    cor= df.corr()\n",
    "    v= sns.heatmap(cor, square = True)\n",
    "    plt.savefig(\"heatmatrix.png\")\n",
    "    mydoc.add_heading(\"The heat matrix of all the features\", 6)\n",
    "    mydoc.add_picture(\"heatmatrix.png\")\n",
    "    kv= df.describe()\n",
    "    kv= kv.transpose()\n",
    "    mydoc.add_heading(\"Detailed description of the data entries\", 6)\n",
    "    mydoc.add_paragraph(str(kv))\n",
    "    mydoc.add_heading(\"Data Completedness: Number of incomplete data entries\", 6)\n",
    "    mydoc.add_paragraph(str(df.isna().sum().sum()))\n",
    "    mydoc.add_heading(\"Data Uniqueness: Number of duplicates\", 6)\n",
    "    mydoc.add_paragraph(str(countduplicated))\n",
    "    mydoc.add_heading(\"Ratio of data to errors:\", 6)\n",
    "    mydoc.add_paragraph(str((df.isna().sum().sum() + countduplicated)/df.shape[0]))\n",
    "    \n",
    "qualityofdata(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-python",
   "metadata": {},
   "source": [
    "### Featurizing the Data <a class=\"anchor\" id=\"fet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-snake",
   "metadata": {},
   "source": [
    "#### Featurize using Matminer <a class=\"anchor\" id=\"featurize\"></a>\n",
    "To  featurize  the  dataframe,  a  python  library  called  matminer  was  used.   Mat-miner is used for data mining the propetries of materials.  It contains routinesfor obtaining data on material properties from various databases, featurizing com-plex materials attributes (e.g., composition, crystal structure, band structure) intophysically-relevant numerical quantities, and analyzing the results of data mining. An example of data featurizing can be seen here:\n",
    "![title](Pictures/featurize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen import Composition #featurizing using matminer\n",
    "mydoc.add_heading('A summary of the data featurizing process', 1)\n",
    "\n",
    "# using the composition\n",
    "try:\n",
    "    from matminer.featurizers.conversions import StrToComposition\n",
    "    stc= StrToComposition()\n",
    "    df= stc.featurize_dataframe(df, \"formula_pretty\")\n",
    "    #df= stc.featurize_dataframe(df, \"formula\")\n",
    "    mydoc.add_paragraph(\"Converted formula into composition\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to convert formula into composition\")\n",
    "        \n",
    "try:\n",
    "    from matminer.featurizers.composition import ValenceOrbital\n",
    "    vo= ValenceOrbital()\n",
    "    df= vo.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Valence Orbitals\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Valence Orbitals\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.composition import BandCenter\n",
    "    bc= BandCenter()\n",
    "    df= bc.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Band Center\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Band Center\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.composition import Stoichiometry\n",
    "    smet= Stoichiometry()\n",
    "    df= smet.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Stoichiometry\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Stoichiometry\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.composition import IonProperty\n",
    "    ip= IonProperty()\n",
    "    df= ip.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Ion Property\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Ion Property\")\n",
    "    \n",
    "try:\n",
    "    from matminer.featurizers.composition import ElementFraction\n",
    "    ef= ElementFraction()\n",
    "    df= ef.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Element Fraction\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Element Fraction\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.conversions import CompositionToOxidComposition\n",
    "    ctoc= CompositionToOxidComposition()\n",
    "    df= ctoc.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Oxidation Composition\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Oxidation Composition\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.composition import OxidationStates\n",
    "    os= OxidationStates()\n",
    "    df= os.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Oxidation States\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Oxidation States\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.composition import YangSolidSolution\n",
    "    mg= YangSolidSolution()\n",
    "    df= mg.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Yang Solid Solution\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Yang Solid Solution\")\n",
    "    \n",
    "try:\n",
    "    from matminer.featurizers.composition import ElectronAffinity\n",
    "    ea= ElectronAffinity()\n",
    "    df= mg.featurize_dataframe(df, \"composition\")\n",
    "    mydoc.add_paragraph(\"Obtained the Electron Affinity\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Electron Affinity\")\n",
    "\n",
    "    \n",
    "    \n",
    "# using the structure\n",
    "try:\n",
    "    from matminer.featurizers.structure import DensityFeatures\n",
    "    denfea= DensityFeatures()\n",
    "    df= denfea.featurize_dataframe(df, \"structure\")\n",
    "    mydoc.add_paragraph(\"Obtained the Density Features\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Density Features\")\n",
    "    \n",
    "try:\n",
    "    from matminer.featurizers.structure import Dimensionality\n",
    "    dms= Dimesionality()\n",
    "    df= dms.featurize_dataframe(df, \"structure\")\n",
    "    mydoc.add_paragraph(\"Obtained the Dimensionality\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Dimensionality\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.structure import EwaldEnergy\n",
    "    eww= EwaldEnergy()\n",
    "    df= eww.featurize_dataframe(df, \"structure\")\n",
    "    mydoc.add_paragraph(\"Obtained the Ewald Energy\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Ewald Energy\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.structure import GlobalInstabilityIndex\n",
    "    gii= GlobalInstabilityIndex()\n",
    "    df= gii.featurize_dataframe(df, \"structure\")\n",
    "    mydoc.add_paragraph(\"Obtained the Global Instability Index\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Global Instability Index\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.structure import MaximumPackingEfficieny\n",
    "    mpe= MaximumPackingEfficiency()\n",
    "    df= mpe.featurize_dataframe(df, \"structure\")\n",
    "    mydoc.add_paragraph(\"Obtained the Maximum Packing Efficiency\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Maximum Packing Efficiency\")\n",
    "\n",
    "\n",
    "\n",
    "# using the site\n",
    "try:\n",
    "    from matminer.featurizers.site import CoordinationNumber\n",
    "    cn= CoordinationNumber()\n",
    "    df= cn.featurize_dataframe(df, \"site\")\n",
    "    mydoc.add_paragraph(\"Obtained the Coordination Number\") \n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Coordination Number\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.site import AverageBondLength\n",
    "    abl= AverageBondLength()\n",
    "    df= abl.featurize_dataframe(df, \"site\")\n",
    "    mydoc.add_paragraph(\"Obtained the Average Bond Length\") \n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Average Bond Length\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.site import AverageBondAngle\n",
    "    aba= AverageBondAngle()\n",
    "    df= aba.featurize_dataframe(df, \"site\")\n",
    "    mydoc.add_paragraph(\"Obtained the Average Bond Angle\") \n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Average Bond Angle\")\n",
    "\n",
    "try:\n",
    "    from matminer.featurizers.site import AGNIFingerprints\n",
    "    afp= AGNIFingerprints()\n",
    "    df= afp.featurize_dataframe(df, \"site\")\n",
    "    mydoc.add_paragraph(\"Obtained the AGNIFingerprints\") \n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the AGNIFingerprints\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-phenomenon",
   "metadata": {},
   "source": [
    "#### Additiontal Manual Featurizing<a class=\"anchor\" id=\"amf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-clear",
   "metadata": {},
   "source": [
    "#### Number of Atoms \n",
    "This functions counts the number of atoms in the molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_atoms(formula):\n",
    "    numberofatoms=[]\n",
    "    \n",
    "    for i in range(len(formula)):\n",
    "        \n",
    "        compound= formula[i]\n",
    "        counter=0\n",
    "        stringcounter=0\n",
    "        numbercounter=0\n",
    "    \n",
    "        for v in range(0, len(compound)):\n",
    "            \n",
    "            if(compound[v].isnumeric()):\n",
    "                counter= counter+ int(compound[v])\n",
    "                numbercounter+=1\n",
    "            else:\n",
    "                if(compound[v].isupper()):\n",
    "                    stringcounter+=1\n",
    "                    \n",
    "        counter= counter + stringcounter - numbercounter\n",
    "        numberofatoms.append(counter)\n",
    "        \n",
    "    return numberofatoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-silicon",
   "metadata": {},
   "source": [
    "#### Number of Magnetic Atoms\n",
    "This function calculates the number of magnetic atoms present in the molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_magnetic_atoms(formula):\n",
    "    \n",
    "    magnetic_atoms= [\"Fe\", \"Co\", \"Dy\", \"Sm\", \"Ni\", \"Gd\", \"Nd\"] #the magnetic atoms of the periodic table\n",
    "    count=0\n",
    "    tr=0\n",
    "    numberofmagneticatoms=[]\n",
    "    \n",
    "    for i in range(len(formula)):\n",
    "        \n",
    "        compound= formula[i]\n",
    "        count=0\n",
    "        \n",
    "        for j in range(0, len(magnetic_atoms)):\n",
    "            \n",
    "            if magnetic_atoms[j] in compound:\n",
    "                pos= compound.index(magnetic_atoms[j])\n",
    "                if(pos+2 != len(compound)): #here each of the magnetic_atoms has a short form of 2 characters\n",
    "                    if(compound[pos+2].isnumeric()):\n",
    "                        count= count + int(compound[pos+2])\n",
    "                    else:\n",
    "                        count= count+1\n",
    "                else:\n",
    "                    count= count+1\n",
    "                    \n",
    "        numberofmagneticatoms.append(count)\n",
    "        \n",
    "    return numberofmagneticatoms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the features manually coded.\n",
    "try:\n",
    "    #numberofatoms= number_of_atoms(df['formula'])\n",
    "    numberofatoms= number_of_atoms(df['formula_pretty'])\n",
    "    mydoc.add_paragraph(\"Obtained the Number of Atoms\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Number of Atoms\")\n",
    "\n",
    "try:\n",
    "    #numberofmagneticatoms= number_of_magnetic_atoms(df['formula'])\n",
    "    numberofmagneticatoms= number_of_magnetic_atoms(df['formula_pretty'])\n",
    "    mydoc.add_paragraph(\"Obtained the Number of Magnetic Atoms\")\n",
    "except:\n",
    "    mydoc.add_paragraph(\"Unable to obtain the Number of Magnetic Atoms\")\n",
    "\n",
    "    \n",
    "df.insert(2, \"number_of_magnetic_atoms\", numberofmagneticatoms, True)\n",
    "df.insert(3, \"number_of_atoms\", numberofatoms, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the doc after the process of data featurizing\n",
    "mydoc.add_heading('Columns after data featurizing', 1)\n",
    "mydoc.add_paragraph(str(df.shape[1]))\n",
    "mydoc.add_paragraph(list(df.columns))\n",
    "mydoc.add_heading('Data Entries', 1)\n",
    "mydoc.add_paragraph(str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-collection",
   "metadata": {},
   "source": [
    "### Data Preprocessing<a class=\"anchor\" id=\"prep\"></a>\n",
    "Real-world data may be inconsistent, noisy and incomplete.  Hence,data preprocessing generates quality data.\n",
    "![title](Pictures/dfdppic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-kingston",
   "metadata": {},
   "source": [
    "### Data Cleaning <a class=\"anchor\" id=\"cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_heading('Summary of Data Preprocessing', 1)\n",
    "mydoc.add_heading('The summary of the data cleaning', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-completion",
   "metadata": {},
   "source": [
    "In the sub-stepdata cleaning, it is crucial to remove data that has no numerical signifance such as data of discovery, text data and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing columns with no numerical significance\n",
    "mydoc.add_heading('Deleting columns with no numerical significance', 6)\n",
    "x= df.select_dtypes(['number'])\n",
    "mydoc.add_heading('Number of columns with no numerical significance', 6)\n",
    "mydoc.add_paragraph(str(df.shape[0]- x.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-danger",
   "metadata": {},
   "source": [
    "In the sub-stepdata cleaning, it is crucial to remove redundant data suchas columns with low variance, that is features which do not vary much withinitself and have little predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting columns with low variance\n",
    "mydoc.add_heading('Deleting columns with low variance', 6)\n",
    "count = (x.nunique())\n",
    "to_del = [i for i,v in enumerate(count) if v == 1]\n",
    "for i in range(len(to_del)):\n",
    "    t= to_del[i]\n",
    "    x.drop(x.columns[t-i], axis=1, inplace=True)\n",
    "\n",
    "df= x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-vintage",
   "metadata": {},
   "source": [
    "#### Data Imputation\n",
    "Data imputation is the process of replacing missing data with substitutedvalues (obtained through methods like regression or clustering).  Often times, missing data entries are replaced with the average value of the entire column. Here, the user is offered a choice about whether  data imputation should be performed or not. Since, this step is very subjective to the quality and the use of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting rows with missing data and duplicate data\n",
    "def dataimputation(df):\n",
    "    df= df.fillna(df.mean())\n",
    "    return df\n",
    "\n",
    "mydoc.add_heading(\"Performing data imputation\", 6)\n",
    "datai= input(\"Enter yes for replacing all missing data with the mean value of the column and no if you want to delete data entries: \")\n",
    "\n",
    "if datai== 'yes':\n",
    "    df= dataimputation(df)\n",
    "    mydoc.add_paragraph(\"All missing data replaced with the mean value of the column\")\n",
    "\n",
    "else:\n",
    "    mydoc.add_paragraph('Deleting data entries with missing data')\n",
    "    original= df.shape[1]\n",
    "    df= df.dropna()\n",
    "    new= df.shape[1]\n",
    "    mydoc.add_paragraph('Number of data entries with missing data which were deleted')\n",
    "    mydoc.add_paragraph(str(original-new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-allergy",
   "metadata": {},
   "source": [
    "Checking for duplicate data points and deleting duplicate data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate data points\n",
    "mydoc.add_heading('Checking for Duplicates', 6)\n",
    "dups= list(df.duplicated())\n",
    "countduplicated=0\n",
    "for i in range (0, len(dups)):\n",
    "    if dups[i]== True:\n",
    "        countduplicated= countduplicated+1\n",
    "if countduplicated==0:\n",
    "    mydoc.add_paragraph('No duplicates present')\n",
    "else:\n",
    "    df.drop_duplicates()\n",
    "    mydoc.add_paragraph('Duplicates were present and they have been dropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-ranking",
   "metadata": {},
   "source": [
    "### Data Transformation<a class=\"anchor\" id=\"transformation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_heading(\"The summary of data transformation\", 1)\n",
    "mydoc.add_heading(\"Conducting data binning\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-vegetable",
   "metadata": {},
   "source": [
    "#### Data binning\n",
    "Thebinningor discretization or concept hierarchy generation method is usedto minimize the effects of small observational errors and smoothen out the data. Data binning groups numbers having continuous values into smaller bins. The number of bins and the width is determined using Sturge’s rule:number of bins = 1 + log2(length of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(lower_bound, width, quantity):\n",
    "    \n",
    "    bins=[]\n",
    "    \n",
    "    for i in range(quantity):\n",
    "        if i == quantity-1:\n",
    "            bins.append((lower_bound+i*width, lower_bound+(i+1)*width+0.1))\n",
    "        else:\n",
    "            bins.append((lower_bound+i*width, lower_bound+(i+1)*width))\n",
    "        \n",
    "    return bins\n",
    "\n",
    "def find_bin(value, bins):\n",
    "    \n",
    "    for i in range(0, len(bins)):\n",
    "        if bins[i][0] <= value < bins[i][1]:\n",
    "            return i\n",
    "        \n",
    "def binning():\n",
    "    \n",
    "    #Using Sturge's Rule for bin calculation\n",
    "    k= math.ceil(1+ math.log2(len(x)))\n",
    "    mydoc.add_paragraph('The number of bins created:')\n",
    "    mydoc.add_paragraph(str(k))\n",
    "    column_names=list(x.columns.values)\n",
    "    binned_x= pd.DataFrame()\n",
    "    print(column_names)\n",
    "    for i in range(len(column_names)):\n",
    "        \n",
    "        if(x.columns[i]==\"total_magnetization\"):\n",
    "            binned_x.insert(0, x.columns[i], x[column_names[i]].tolist(), True)\n",
    "            continue\n",
    "        columns_into_list=[]\n",
    "        columns_into_list= x[column_names[i]].values\n",
    "        lower_bound= min(columns_into_list)\n",
    "        #print(lower_bound)\n",
    "        upper_bound= max(columns_into_list)\n",
    "        #print(upper_bound)\n",
    "        width= (upper_bound - lower_bound)/k\n",
    "        bins= create_bins(lower_bound, width, k)\n",
    "\n",
    "        t=[]\n",
    "        for j in range (len(columns_into_list)):\n",
    "            t.append(find_bin(columns_into_list[j], bins))\n",
    "        \n",
    "        binned_x.insert(0, x.columns[i], t, True)\n",
    "    \n",
    "    return binned_x\n",
    "\n",
    "x=df\n",
    "df= binning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-optimum",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "The goal of data normalizationis to change the values of numeric columnsin the data set to a common scale, without distorting differences in the rangesof values.  This ensures that one attribute is not given greater importance justby virtue of higher magnitudes of the attribute entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_heading(\"Conducting data normalisation\", 6)\n",
    "k= math.ceil(1+ math.log2(len(x)))\n",
    "df= df.div(k)\n",
    "mydoc.add_paragraph(\"Values in all the columns are now in the range of 0 to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-organization",
   "metadata": {},
   "source": [
    "### Data Reduction <a class=\"anchor\" id=\"reduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-while",
   "metadata": {},
   "source": [
    "#### Numerosity Reduction\n",
    "The code gives the option to reduce the number of data entries. In case, the data is extremely huge and it could exponentially increase the time and resources of the machine learning algorithms there would be an algorithm in place to reduce the data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_heading('Summary of Data Reduction', 1)\n",
    "mydoc.add_heading('Conducting Numerosity Reduction', 6)\n",
    "reduction= input(\"Fraction of Data to be Kept (0 for nothing, 1 for all): \")\n",
    "mydoc.add_paragraph(\"Fraction of Numerosity Reduction by\")\n",
    "mydoc.add_paragraph(reduction)\n",
    "df= df.sample(frac=float(reduction), replace=False, random_state=1)\n",
    "mydoc.add_paragraph(\"Number of data entries reduced\")\n",
    "mydoc.add_paragraph(str(x.shape[0]-df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-reduction",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "In a machine learning model, each feature shouldbe independent otherwise the problem of multi-collinearity that leads to re-dundant variables in the machine learning model arises.  Hence, a pair-plot isused to plot pairwise relationships of each attribute in a dataset.  The pair-plots which depict a relation(for instance:  linear, exponential etc) should be investigated and appropriate attribute selection must be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_heading('Conducting Dimensionality Reduction', 6)\n",
    "dftemp= pd.DataFrame.from_dict(materials)\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "g = sns.pairplot(dftemp)\n",
    "g.savefig(\"pairplot.png\")\n",
    "mydoc.add_paragraph(\"The graph of dependence of attributes on each other\")\n",
    "mydoc.add_picture(\"pairplot.png\")\n",
    "mydoc.add_paragraph(\"Action should be taken after a manual visual inspection of the plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-mileage",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "While ideally clusters should be available for each pair of features, that would result in 156C2 graphs. Hence, a feature to obtain as many number of graphs as needed has been given in the code, along with the number of clusters wanted. That is the user can define how many cluster plots he/she would like to see, after that he/she would enter the rows and columns of each of those cluster plots and the number of clusters they would like to see in the plot. This is a method of manual inspection of the data. An example of the clustering can be seen below, where the data is divided into four clusters, each red dot being the centroid of one of the clusters. In this particular example nothing can be narrowed done from clustering, like was the case with many data pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_heading('Clustering the Data', 6)\n",
    "from sklearn.cluster import KMeans\n",
    "def clustering(column1, column2, noofclusters):\n",
    "    c1= df[column1]\n",
    "    c2= df[column2]\n",
    "    temp= pd.DataFrame(list(zip(c1, c2)), columns=['x','y'])\n",
    "    kmeans = KMeans(n_clusters=noofclusters).fit(temp)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(temp['x'], temp['y'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n",
    "    plt.xlabel(\"bandgap\")\n",
    "    plt.ylabel(\"energy_vdw_per_atom\")\n",
    "    plt.savefig(\"clusterplot.png\")\n",
    "    mydoc.add_picture(\"clusterplot.png\")\n",
    "\n",
    "noofclusterplots= input(\"Number of cluster plots wanted:\")\n",
    "for i in range(0, int(noofclusterplots)):\n",
    "    col1= input(\"Enter column name 1:\")\n",
    "    col2= input(\"Enter column name 2:\")\n",
    "    dataframecolumns= df.columns\n",
    "    poscol1=-1\n",
    "    poscol2=-1\n",
    "    for i in range (0, len(dataframecolumns)):\n",
    "        if dataframecolumns[i]== col1:\n",
    "            poscol1= i\n",
    "            break\n",
    "            \n",
    "    for i in range (0, len(dataframecolumns)):\n",
    "        if dataframecolumns[i]== col2:\n",
    "            poscol2= i\n",
    "            break\n",
    "            \n",
    "    noofclusters= input(\"Enter no of clusters\")\n",
    "    clustering(col1, col2, int(noofclusters))\n",
    "    mydoc.add_paragraph(\"Created above cluster plot for following columns\")\n",
    "    mydoc.add_paragraph(str(col1))\n",
    "    mydoc.add_paragraph(str(col2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-cheat",
   "metadata": {},
   "source": [
    "End of the process of data featurizing and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the quality of data after the entire process of data featurizing an data preprocessing.\n",
    "qualityofdata(df)\n",
    "mydoc.add_heading(\"End of Data Featurizing and Data Pre-processing\")\n",
    "mydoc.add_paragraph(\"Data is now ready for input in machine learning and deep learning algorithms\")\n",
    "mydoc.save(\"DFDP-Results.docx\")\n",
    "df.to_csv(\"DataAfterDFDP.CSV\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
