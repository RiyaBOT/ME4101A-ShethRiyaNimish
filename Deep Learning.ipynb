{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charitable-produce",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Sheth Riya Nimish\n",
    "### A0176880R\n",
    "e0235287@u.nus.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-wireless",
   "metadata": {},
   "source": [
    "#### This is the code that is used for data featurizing and data preprocessing. The code has been written entirely from scratch by me with the use of existing python libraries like keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-wholesale",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "\n",
    "* [Importing the Required Libraries](#import)\n",
    "* [Creating a Document to Store the Results](#create)\n",
    "* [Retrieval of Data](#ret)\n",
    "* [Sequential Model](#seq)\n",
    "* [Hyperparameter Optimisation](#hpo)\n",
    "* [Functional Model](#func)\n",
    "* [Explainability](#ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-inspector",
   "metadata": {},
   "source": [
    "### Importing the Required Libraries<a class=\"anchor\" id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import docx\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-steal",
   "metadata": {},
   "source": [
    "### Creating a Document to Store the Results of Data Featurizing and Data Preprocessing <a class=\"anchor\" id=\"create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc= docx.Document()\n",
    "mydoc.add_heading(\"Deep Learning\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-hartford",
   "metadata": {},
   "source": [
    "### Retrieval of Data <a class=\"anchor\" id=\"ret\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=input(\"Filename:\")\n",
    "df= pd.read_csv(filename)\n",
    "y= df.iloc[:, -1:]\n",
    "lendf= len(df.columns)\n",
    "x= df.drop(df.columns[lendf-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting of the data to get a good test \n",
    "X_train, X_test, y_train, y_test= train_test_split(x, y, test_size= 0.2)\n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test = np.asarray(y_test).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-privilege",
   "metadata": {},
   "source": [
    "### Sequential Model <a class=\"anchor\" id=\"seq\"></a>\n",
    "A simple deep learning model called the Sequential model which is a linear stack of layers.\n",
    "This function creates a sequential model based on the parameters number of layers, number of nodes in neurons in each of the layers, the batchsize and the number of epochs. Besides computing the accuracy of the model, other graphs such as the model accuracy vs the epochs and the model loss vs the epochs is also plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_paragraph(\"Sequential Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SequentialModel(numberoflayers, numberofneurons, batch, numberofepochs):\n",
    "    mydoc.add_heading(\"Creating Sequential Model\")\n",
    "    mydoc.add_paragraph(\"Number of Layers\")\n",
    "    mydoc.add_paragraph(str(numberoflayers))\n",
    "    mydoc.add_paragraph(\"Number of Neurons in each layer\")\n",
    "    mydoc.add_paragraph(str(numberofneurons))\n",
    "    mydoc.add_paragraph(\"Batch Size\")\n",
    "    mydoc.add_paragraph(str(batch))\n",
    "    mydoc.add_paragraph(\"Number of Epochs\")\n",
    "    mydoc.add_paragraph(str(numberofepochs))\n",
    "    classifier= Sequential()\n",
    "    classifier.add(Dense(numberofneurons[0], kernel_initializer = \"uniform\",activation = \"relu\", input_dim=X_train.shape[1]))\n",
    "    for i in range(1, (numberoflayers)):\n",
    "        classifier.add(Dense(numberofneurons[i], kernel_initializer = \"uniform\",activation = \"sigmoid\"))\n",
    "    classifier.compile(optimizer= \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "    classifier.summary()\n",
    "    history= classifier.fit(X_train, y_train, batch_size = batch, epochs = numberofepochs)\n",
    "    prediction= classifier.predict(X_test)\n",
    "    #pred= (prediction.tolist())\n",
    "    #print(pred)\n",
    "    #matrix = metrics.confusion_matrix(y_test, prediction)\n",
    "    #print(matrix)\n",
    "    #mydoc.add_paragraph(str(matrix))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig(\"Pictures\\dlma.png\")\n",
    "    mydoc.add_picture(\"Pictures\\dlma.png\")\n",
    "    plt.clf()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig(\"Pictures\\dlml.png\")\n",
    "    mydoc.add_picture(\"Pictures\\dlml.png\")\n",
    "    loss, accuracy = classifier.evaluate(X_test, y_test, verbose=0)\n",
    "    #print(accuracy)\n",
    "    return loss, accuracy, classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-gibson",
   "metadata": {},
   "source": [
    "### Hyper Parameter Optimisation <a class=\"anchor\" id=\"hpo\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-faculty",
   "metadata": {},
   "source": [
    "Optimising the number of layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberoflayers= [3, 5, 7, 9, 11, 15, 21, 23, 30, 40, 50] \n",
    "numberofneurons=[]\n",
    "for i in range (50):\n",
    "    numberofneurons.append(1)\n",
    "numberofneurons[0]= 9\n",
    "\n",
    "accuracylist=[]\n",
    "for i in range(len(numberoflayers)):\n",
    "    loss, accuracy, classifier= SequentialModel(numberoflayers[i], numberofneurons, 10, 100)\n",
    "    accuracylist.append(accuracy)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(numberoflayers, accuracylist)\n",
    "plt.xlabel(\"Number of Layers\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of Layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-spice",
   "metadata": {},
   "source": [
    "Optimising the three input parameters, batchsize number of layers, number of neurons simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize= [5, 10, 15, 20, 25, 40, 50]\n",
    "numberoflayers= [3, 5, 7, 9, 11, 15, 21, 23, 30, 40, 50] \n",
    "plotnumberofneurons=[1, 3, 5, 10, 20, 50]\n",
    "accuracylist=[]\n",
    "numberofneurons=[]\n",
    "\n",
    "for i in range (len(batchsize)):\n",
    "    for j in range (len(numberoflayers)):\n",
    "        for k in range (len(plotnumberofneurons)):\n",
    "            numberofneruons=[]\n",
    "            for l in range(numberoflayers[j]):\n",
    "                numberofneurons.append(1)\n",
    "            numberofneurons[0]= plotnumberofneurons[k]\n",
    "            print(numberofneurons)\n",
    "            loss, accuracy, clf= SequentialModel(numberoflayers[j], numberofneurons, batchsize[i], 50)\n",
    "            accuracylist.append((numberoflayers[j], plotnumberofneurons[k], batchsize[i], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimising the batch size of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize= [5, 10, 15, 20, 25, 40, 50]\n",
    "accuracylist=[]\n",
    "\n",
    "for i in range(len(batchsize)):\n",
    "    loss, accuracy= SequentialModel(3, [9, 1, 1], batchsize[i], 50)\n",
    "    accuracylist.append(accuracy)\n",
    "plt.clf()\n",
    "plt.plot(batchsize, accuracylist)\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Batch Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-services",
   "metadata": {},
   "source": [
    "Optimising the number of nodes in the neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberoflayers= 3\n",
    "numberofneurons= [(1, 1, 1), (3, 1, 1), (5, 1, 1), (10, 1, 1), (20, 1, 1), (50, 1, 1)]\n",
    "plotnumberofneurons=[1, 3, 5, 10, 20, 50]\n",
    "accuracylist=[]\n",
    "for i in range(len(numberofneurons)):\n",
    "    loss, accuracy, clf= SequentialModel(numberoflayers, numberofneurons[i], 5, 50)\n",
    "    accuracylist.append(accuracy)\n",
    "plt.clf()\n",
    "plt.plot(plotnumberofneurons, accuracylist)\n",
    "plt.xlabel(\"Number of Neurons in Each Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of Neurons in Each Layer\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-computer",
   "metadata": {},
   "source": [
    "The best model obtained after optimising and fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, model= SequentialModel(3, [50, 1, 1], 5, 100) #these values were obtained after optimisation\n",
    "mydoc.add_paragraph(\"Best Model Results\")\n",
    "mydoc.add_paragraph(\"Accuracy\")\n",
    "mydoc.add_paragraph(str(accuracy))\n",
    "mydoc.add_paragraph(\"Loss\")\n",
    "mydoc.add_paragraph(str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-spray",
   "metadata": {},
   "source": [
    "### Functional Model <a class=\"anchor\" id=\"func\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_paragraph(\"Functional Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-america",
   "metadata": {},
   "source": [
    "#### Best Functional Model\n",
    "The optimisation of the functional model can be done using the same code in the sequential model with a few modifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(X_train.shape[1],))\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.2)\n",
    "\n",
    "test_scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])\n",
    "mydoc.add_paragraph(\"Accuracy\")\n",
    "mydoc.add_paragraph(str(test_scores[1]))\n",
    "mydoc.add_paragraph(\"Loss\")\n",
    "mydoc.add_paragraph(str(test_scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc.add_paragraph(\"\")\n",
    "mydoc.save(\"DeepLearning.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-tuner",
   "metadata": {},
   "source": [
    "### Explainability of the Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-draft",
   "metadata": {},
   "source": [
    "Explaning the model using two post-hoc algorithms, SHAP and LIME.\n",
    "Uncomment below for explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.KernelExplainer(model.predict, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"logit\")\n",
    "shap.summary_plot(shap_values, x)\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=\"logit\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "predict_fn_rf = lambda x: model.predict(x).astype(float)\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(x.values, feature_names= x.columns)\n",
    "\n",
    "chosen_instance = x.loc[10].values\n",
    "print(chosen_instance)\n",
    "exp = explainer.explain_instance(chosen_instance, predict_fn_rf)\n",
    "exp.show_in_notebook(show_all=False)\n",
    "exp.show_in_notebook(show_all=False)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
